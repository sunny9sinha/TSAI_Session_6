
## END 2 Phase 1 Assignment 6 - How to make encoder-decoder classes
------------------------------------------------------------------------------------------------------------

## Group : 
1. Sunny Sinha
2. Pratik Jain
3. Anudeep

----------------------
## Notes 
---------------------------------------------------------------------------------------------------------------------------

## Question
* Make 2 seperate classes for encoder, decoder and 1 class for combining logic
* Design the network so that it follows following logic flow:

  - embedding
  - word from a sentence +last hidden vector -> encoder -> single vector
  - single vector + last hidden vector -> decoder -> single vector
  - single vector -> FC layer -> Prediction

## Solution
* Created seperate classes for Encoder and Decoder and a 3rd class which combines the seperate outputs of encoder and decoder

#### Encoder
* Takes sentence text -> converts to embeddings -> runs through LSTM -> get final hidden vector(context vector representing full sentence information) as output
```python
class encoders(nn.Module):

  def __init__(self,vocab_size, embedding_dim, hidden_dim, n_layers, dropout):
    super().__init__()
    self.embedding = nn.Embedding(vocab_size,embedding_dim)
    self.encoder = nn.LSTM(embedding_dim, 
                           hidden_dim, 
                           num_layers=n_layers, 
                           dropout=dropout,
                           batch_first=True)
    
  def forward(self,text,text_lengths):
    embedded = self.embedding(text)
    packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu(), batch_first=True)
    packed_output, (hidden, cell) = self.encoder(packed_embedded)
    return packed_output, hidden
````

#### Decoder
* Defined as sequence of individual lstm cells which runs in a loop for specified no of times
* Takes encoder hidden vector as input
* This input vector is sent to sequence of lstm cells( same input vector for each cell along with last lstm cell hidden output)
* For first lstm cell hidden and cell states are initialized to 0
* Final lstm cell's hidden output vector is the output from Decoder
```python
class decoders(nn.Module):

  def __init__(self, input_to_decoder_size, decoder_hidden_size, no_times_decoder_cell_has_to_run):
    super().__init__()
    self.decoder_single_rnn_cell = nn.LSTMCell(input_to_decoder_size,decoder_hidden_size)
    self.no_times_decoder_cell_has_to_run = no_times_decoder_cell_has_to_run
    self.decoder_hidden_size = decoder_hidden_size

  def forward(self, encoder_context_vector):
    encoder_context_vector = encoder_context_vector.squeeze(0)
    hx = torch.zeros(encoder_context_vector.size(0),self.decoder_hidden_size).to(device)
    cx = torch.zeros(encoder_context_vector.size(0),self.decoder_hidden_size).to(device)
    otpt = []
    for i in range(self.no_times_decoder_cell_has_to_run):
      hx,cx = self.decoder_single_rnn_cell(encoder_context_vector,(hx,cx))
      otpt.append(hx)
      # print(i,hx.shape)
    otpt = torch.stack(otpt,dim = 0)
    return otpt, hx
```

#### Combining encoder and decoder classes outputs
* Created a 3rd class to design the full logic and use encoder/decoder outputs
* In this class text is taken as input -> passed to encoder ->encoder output passed to decoder -> decoder output passed to fully connected layer to give output of size equals to no of classification classes.
```python
class classifier(nn.Module):
    
    # Define all the layers used in model
    def __init__(self, encoder, decoder, hidden_dim, output_dim):
        
      super().__init__()
      self.encoder = encoder
      self.decoder = decoder
      self.fc = nn.Linear(hidden_dim, output_dim)
        
    def forward(self,src,src_len):
      # print('combined')
      # print(src.shape)
      enc_packed_outputs, enc_hidden = self.encoder(src,src_len)
      # print(enc_hidden.shape)
      dec_otpt, dec_hidden = self.decoder(enc_hidden)
      # print(dec_hidden.shape)
      dense_outputs = self.fc(dec_hidden)
      #print(dense_outputs.shape)
      op = F.softmax(dense_outputs, dim =1)
      # return dense_outputs
      return op
 ```
 
 ## Training logs
```python
         Train Loss: 1.072 | Train Acc: 55.04%
	 Val. Loss: 1.019 |  Val. Acc: 73.88% 

	Train Loss: 0.996 | Train Acc: 59.99%
	 Val. Loss: 0.854 |  Val. Acc: 76.12% 

	Train Loss: 0.899 | Train Acc: 66.41%
	 Val. Loss: 0.815 |  Val. Acc: 73.44% 

	Train Loss: 0.799 | Train Acc: 75.93%
	 Val. Loss: 0.845 |  Val. Acc: 68.53% 

	Train Loss: 0.732 | Train Acc: 82.01%
	 Val. Loss: 0.823 |  Val. Acc: 70.54% 

	Train Loss: 0.691 | Train Acc: 86.01%
	 Val. Loss: 0.783 |  Val. Acc: 75.67% 

	Train Loss: 0.675 | Train Acc: 87.30%
	 Val. Loss: 0.825 |  Val. Acc: 71.43% 

	Train Loss: 0.656 | Train Acc: 89.06%
	 Val. Loss: 0.780 |  Val. Acc: 76.56% 

	Train Loss: 0.647 | Train Acc: 89.89%
	 Val. Loss: 0.785 |  Val. Acc: 76.34% 

	Train Loss: 0.643 | Train Acc: 90.38%
	 Val. Loss: 0.789 |  Val. Acc: 75.22% 
```

## Printing the outputs for each step for encoder and decoder

#### Example 1 -> "I didn't like it."
```
classify_tweet("I didn't like it.")

Encoder LSTM output vector after each word: 
after 1 word
tensor([ 3.3666e-01, -1.1060e-01,  1.4279e-01,  1.9930e-02,  9.4484e-02,
         4.4644e-01, -2.4611e-01, -1.7161e-01, -2.7330e-01,  4.9464e-02,
         2.5922e-01,  5.5827e-02,  1.2597e-01,  4.6192e-02,  4.5015e-03,
        -8.1919e-02,  3.7158e-02, -3.1237e-02,  2.8741e-01,  6.7959e-02,
        -1.2523e-01, -4.4651e-01,  2.1111e-01,  1.5342e-01, -2.4255e-01,
         3.6058e-02,  2.7419e-02, -6.2437e-02,  5.6175e-02,  7.1488e-03,
        -2.9687e-02, -2.6871e-02,  5.2023e-02, -2.3182e-01,  7.6664e-02,
        -3.7827e-01,  4.4040e-02, -1.4775e-01,  1.1352e-01,  4.8270e-02,
         2.3656e-02, -1.4615e-01, -7.3957e-02,  9.2659e-02,  4.1937e-05,
         3.0000e-03,  2.4108e-04, -1.8857e-02, -3.3915e-01, -7.7533e-02,
         2.1053e-03,  2.4549e-03,  1.7717e-02,  1.0689e-01,  7.8248e-03,
        -1.4617e-01,  1.6179e-02, -4.7919e-02,  2.5926e-02, -1.0161e-01,
        -6.9049e-02,  3.6792e-01,  1.5034e-01, -5.1396e-02, -8.8566e-02,
         2.4957e-01,  1.0844e-02,  2.3884e-01, -1.7515e-01, -1.0444e-01,
        -1.4494e-01,  3.0042e-02, -1.8443e-01, -9.0609e-02,  1.3161e-02,
        -3.3015e-02,  1.4670e-01, -1.6118e-01,  4.4837e-01,  3.8184e-02,
        -1.4525e-01, -1.0441e-01, -7.6025e-02, -1.1184e-01, -1.8584e-02,
        -2.1834e-01, -4.1658e-02, -3.4857e-02, -5.6922e-02, -2.5961e-02,
        -2.1431e-01,  1.3401e-01, -3.5050e-01,  7.4110e-02, -1.4001e-01,
        -9.2997e-02,  6.2108e-02, -8.3995e-02,  1.8370e-01, -1.5952e-01],
       device='cuda:0', grad_fn=<SelectBackward>)
after 2 word
tensor([ 0.1121, -0.2832, -0.2419,  0.0087, -0.0827,  0.3988, -0.0265, -0.0840,
        -0.1305,  0.1213,  0.1439, -0.0108, -0.0194, -0.2527, -0.1917, -0.2112,
        -0.0662,  0.0557, -0.0216,  0.3047, -0.1101, -0.1966,  0.2165,  0.1081,
        -0.2328,  0.0323,  0.0193, -0.0294, -0.0952,  0.0115, -0.0098,  0.1644,
         0.2371, -0.1700,  0.0768, -0.1221, -0.1175, -0.3885,  0.0083,  0.0224,
        -0.1002, -0.3694,  0.0777, -0.0978,  0.1861,  0.4215, -0.1386, -0.1191,
        -0.1090, -0.1507, -0.1584, -0.0328, -0.1613,  0.2731,  0.2827, -0.0131,
        -0.1495, -0.2769, -0.0577, -0.3784,  0.1323,  0.0494,  0.0157,  0.0799,
        -0.1239,  0.0951,  0.4401,  0.2477, -0.0595,  0.0548, -0.1434, -0.2769,
        -0.2746,  0.0760, -0.1161, -0.1397,  0.2824, -0.0794,  0.1502,  0.1669,
        -0.1638, -0.2985,  0.4816, -0.0526,  0.0963,  0.0026, -0.0828,  0.0323,
         0.1673,  0.4068, -0.0462,  0.2973, -0.1730,  0.0153, -0.0213, -0.0682,
         0.0980, -0.1854,  0.1193,  0.0289], device='cuda:0',
       grad_fn=<SelectBackward>)
after 3 word
tensor([ 0.0180, -0.0459, -0.1290, -0.0566,  0.0313,  0.0468, -0.0169,  0.0582,
         0.0149,  0.0582,  0.2255, -0.1976,  0.0308, -0.2558,  0.1403, -0.4391,
        -0.0443,  0.1467, -0.0217,  0.1758, -0.1935, -0.0097,  0.0838,  0.2601,
        -0.4075, -0.1386, -0.0741, -0.2790,  0.0382,  0.4916,  0.1820, -0.2172,
         0.0870, -0.1843,  0.1350, -0.3924,  0.1154, -0.1155,  0.2974, -0.0071,
        -0.1682, -0.0900,  0.1997, -0.1083,  0.1968,  0.0613, -0.2259, -0.0876,
        -0.2769, -0.5252,  0.0820,  0.2765, -0.2754,  0.1187,  0.0600, -0.1242,
        -0.0995,  0.0435, -0.0579, -0.1189, -0.2249,  0.1751,  0.1033, -0.0169,
         0.2372,  0.1177,  0.1245,  0.0057,  0.2317, -0.1346, -0.1328, -0.0342,
        -0.0013,  0.0657, -0.0978, -0.0553, -0.1633, -0.2522,  0.0278, -0.0038,
         0.0305,  0.0164, -0.0517, -0.0253,  0.3601,  0.0403,  0.0198, -0.1945,
         0.0078,  0.3026,  0.3821,  0.1045, -0.3897,  0.1444, -0.0657, -0.0511,
         0.0477, -0.2244, -0.0165,  0.0329], device='cuda:0',
       grad_fn=<SelectBackward>)
after 4 word
tensor([-0.0148, -0.1326,  0.0628, -0.0835, -0.2729,  0.1203, -0.1727,  0.1170,
         0.0056, -0.1305,  0.4516, -0.0302, -0.0487, -0.1230, -0.0345, -0.0293,
         0.1522,  0.1165, -0.0227,  0.0318, -0.0441, -0.0324,  0.4959,  0.0254,
        -0.1793, -0.5649,  0.4295, -0.2718,  0.0478,  0.0650,  0.0888, -0.0460,
         0.1650,  0.0073,  0.2909, -0.1585,  0.0731,  0.0710,  0.0724,  0.0350,
        -0.1025,  0.0009, -0.1514,  0.0679,  0.1650,  0.1084, -0.3045,  0.0913,
        -0.3523, -0.4048, -0.1088,  0.0135, -0.0401,  0.1217, -0.0994, -0.2727,
        -0.1783,  0.3064, -0.2448, -0.0529, -0.0428,  0.2240,  0.2068,  0.0146,
         0.1965,  0.4105,  0.1375,  0.0184,  0.2045, -0.1453, -0.3266, -0.1790,
        -0.1076, -0.1123, -0.0423,  0.0611, -0.1203, -0.2683,  0.0981, -0.1776,
         0.1056,  0.2974, -0.0837,  0.2225,  0.2384,  0.0798,  0.1512, -0.1577,
        -0.2074,  0.1628,  0.1532, -0.0024, -0.4787,  0.0695,  0.0177, -0.4031,
        -0.1149, -0.5635, -0.4296,  0.0783], device='cuda:0',
       grad_fn=<SelectBackward>)
after 5 word
tensor([ 0.0805,  0.0538,  0.3482, -0.1301,  0.0616,  0.0894,  0.1193,  0.0721,
         0.0194,  0.1114,  0.0979,  0.0394,  0.1036, -0.0371,  0.0052, -0.0351,
         0.0854, -0.0574,  0.0199, -0.0134, -0.0378,  0.0156, -0.0880, -0.0231,
        -0.1135,  0.0424,  0.1327, -0.0761,  0.1313,  0.1398,  0.1180,  0.0814,
         0.0653, -0.0387,  0.0956, -0.2677,  0.0271,  0.1095, -0.2414, -0.0781,
         0.0215,  0.0214,  0.0246,  0.2037,  0.2588,  0.5166,  0.0205,  0.3000,
        -0.0059, -0.2284,  0.0619,  0.0571, -0.2099,  0.1996, -0.1115,  0.2774,
        -0.0932, -0.0234,  0.3363,  0.0168, -0.1947, -0.0119,  0.1368,  0.0835,
         0.0475,  0.4798,  0.1721, -0.1966, -0.2454, -0.1200, -0.0294,  0.2783,
        -0.0523, -0.0296,  0.0872,  0.1036, -0.0588, -0.2112,  0.2224, -0.2837,
         0.3602,  0.2230, -0.0877,  0.0101, -0.0069, -0.0388,  0.1697, -0.2595,
        -0.0902,  0.0534,  0.0766,  0.0988, -0.1101, -0.2328, -0.0627, -0.3504,
        -0.3349,  0.0612, -0.0859,  0.0935], device='cuda:0',
       grad_fn=<SelectBackward>)
after 6 word
tensor([-2.3765e-01,  6.0762e-02,  1.6458e-01,  1.4612e-01, -2.7881e-02,
        -8.0440e-02,  1.7792e-01, -1.5610e-01,  2.4000e-01,  8.5250e-02,
        -1.2861e-01, -4.0589e-01, -1.1650e-01,  1.3120e-01,  1.5669e-01,
         2.6729e-01,  7.0901e-02, -1.2359e-01, -1.2888e-01, -5.7525e-02,
         1.3571e-02,  2.0105e-01, -1.8490e-01,  5.1649e-02, -4.8141e-01,
         3.6103e-01,  2.5537e-01, -1.9080e-01,  2.6534e-04, -8.6304e-03,
         1.7448e-03,  2.7085e-01,  4.1172e-02,  1.7020e-01,  1.3293e-01,
        -1.2585e-01,  5.9839e-02, -3.0116e-02, -3.9701e-01, -6.6725e-02,
        -9.0890e-02, -1.4122e-01, -2.2008e-01,  4.7471e-02,  8.0877e-02,
        -7.4657e-03,  1.8479e-01,  2.3903e-01, -3.5251e-01, -1.1148e-01,
        -4.2986e-02, -2.0446e-02,  8.3485e-02,  2.5350e-01,  1.3016e-01,
        -6.2516e-02, -1.1301e-01,  5.8724e-02,  8.6036e-02,  4.9085e-02,
        -1.4918e-01,  1.2330e-01,  5.0827e-02,  7.4364e-04, -1.6372e-02,
         3.6649e-01, -9.0798e-03, -5.3674e-01, -1.0576e-01, -1.8815e-02,
        -1.7332e-01, -1.3033e-01, -1.0174e-02,  1.1173e-01, -1.8992e-01,
         2.2418e-01,  5.8168e-02, -4.5129e-02, -2.4466e-01, -2.6477e-01,
        -1.1906e-02, -9.8164e-02, -8.2492e-02,  3.4140e-03,  2.7759e-03,
        -2.8737e-02,  3.2844e-01,  4.1484e-03, -2.6332e-02,  9.4613e-03,
         7.9007e-03,  1.3705e-01,  3.9097e-02, -2.3248e-01, -7.6254e-02,
        -4.6025e-02, -2.9592e-02,  4.2015e-02,  3.5622e-02,  5.0949e-02],
       device='cuda:0', grad_fn=<SelectBackward>)

Encoder LSTM final hidden vector
tensor([[[-2.3765e-01,  6.0762e-02,  1.6458e-01,  1.4612e-01, -2.7881e-02,
          -8.0440e-02,  1.7792e-01, -1.5610e-01,  2.4000e-01,  8.5250e-02,
          -1.2861e-01, -4.0589e-01, -1.1650e-01,  1.3120e-01,  1.5669e-01,
           2.6729e-01,  7.0901e-02, -1.2359e-01, -1.2888e-01, -5.7525e-02,
           1.3571e-02,  2.0105e-01, -1.8490e-01,  5.1649e-02, -4.8141e-01,
           3.6103e-01,  2.5537e-01, -1.9080e-01,  2.6534e-04, -8.6304e-03,
           1.7448e-03,  2.7085e-01,  4.1172e-02,  1.7020e-01,  1.3293e-01,
          -1.2585e-01,  5.9839e-02, -3.0116e-02, -3.9701e-01, -6.6725e-02,
          -9.0890e-02, -1.4122e-01, -2.2008e-01,  4.7471e-02,  8.0877e-02,
          -7.4657e-03,  1.8479e-01,  2.3903e-01, -3.5251e-01, -1.1148e-01,
          -4.2986e-02, -2.0446e-02,  8.3485e-02,  2.5350e-01,  1.3016e-01,
          -6.2516e-02, -1.1301e-01,  5.8724e-02,  8.6036e-02,  4.9085e-02,
          -1.4918e-01,  1.2330e-01,  5.0827e-02,  7.4364e-04, -1.6372e-02,
           3.6649e-01, -9.0798e-03, -5.3674e-01, -1.0576e-01, -1.8815e-02,
          -1.7332e-01, -1.3033e-01, -1.0174e-02,  1.1173e-01, -1.8992e-01,
           2.2418e-01,  5.8168e-02, -4.5129e-02, -2.4466e-01, -2.6477e-01,
          -1.1906e-02, -9.8164e-02, -8.2492e-02,  3.4140e-03,  2.7759e-03,
          -2.8737e-02,  3.2844e-01,  4.1484e-03, -2.6332e-02,  9.4613e-03,
           7.9007e-03,  1.3705e-01,  3.9097e-02, -2.3248e-01, -7.6254e-02,
          -4.6025e-02, -2.9592e-02,  4.2015e-02,  3.5622e-02,  5.0949e-02]]],
       device='cuda:0', grad_fn=<CudnnRnnBackward>)

Decoder:  tensor([[[ 3.0559e-03,  9.3819e-02,  3.1243e-02, -2.5202e-02,  8.6832e-03,
          -7.9231e-04,  4.7604e-03, -4.8356e-02,  3.3160e-02,  2.4071e-03,
           5.1840e-02,  2.1505e-02,  8.0849e-03,  2.2530e-02,  1.3296e-02,
           7.4944e-03, -3.0867e-02,  6.1577e-02, -4.0822e-02,  8.2007e-03,
           6.6120e-03,  4.1571e-02,  2.0167e-02,  3.1236e-02,  1.4359e-02,
          -1.1691e-02,  3.1467e-02,  2.9330e-02, -3.2617e-02,  1.5917e-02,
           1.6939e-02,  2.3364e-02,  4.8886e-03,  4.3568e-02, -4.2290e-02,
           1.2098e-02, -2.5326e-02,  5.5434e-02, -5.0397e-03, -1.6297e-02,
          -4.6134e-02,  6.4623e-02, -1.0759e-02,  2.2804e-02,  1.6718e-02,
           9.9457e-03, -4.5110e-02, -2.0678e-02, -2.2289e-02, -6.1409e-03,
           6.5940e-02, -4.2870e-02, -2.0943e-02,  3.1315e-02, -3.8670e-03,
           2.8711e-03, -8.1311e-02,  4.4828e-02, -1.1887e-03, -3.7827e-02,
           1.4087e-02,  3.4720e-02,  2.1888e-02,  6.8351e-02,  4.1627e-03,
           1.8875e-02,  6.8299e-03,  1.4354e-02, -8.8326e-03, -6.1710e-03,
          -3.7347e-02, -2.1245e-02, -9.9863e-04,  2.8537e-02, -1.0843e-02,
           6.4472e-02, -1.7490e-02, -9.2339e-03,  8.1166e-02, -1.7857e-02,
          -2.2366e-02,  1.7607e-02, -1.9515e-02,  4.0352e-02, -5.4647e-02,
          -6.0051e-03, -1.7550e-03, -1.0775e-03, -4.1111e-02, -2.4431e-02,
           1.3722e-04, -5.8461e-02, -1.4559e-02,  4.4475e-03,  1.5356e-02,
          -1.6369e-02, -2.7293e-02,  9.4429e-03,  9.8604e-03,  4.0596e-02]],

        [[ 1.0344e-02,  1.4434e-01,  5.2229e-02, -3.3587e-02,  1.1691e-02,
           2.2228e-04,  1.9497e-03, -6.9733e-02,  4.9759e-02,  4.9701e-03,
           7.7794e-02,  3.7625e-02,  8.6321e-03,  2.7833e-02,  2.4825e-02,
           6.7621e-03, -4.3370e-02,  9.1615e-02, -5.7152e-02,  1.5063e-02,
           7.7316e-03,  7.2240e-02,  2.3565e-02,  4.6613e-02,  1.6681e-02,
          -1.8425e-02,  5.2032e-02,  2.9543e-02, -4.3904e-02,  3.4526e-02,
           2.6918e-02,  3.2910e-02,  9.3258e-03,  6.0770e-02, -6.8713e-02,
           2.3326e-02, -2.9118e-02,  8.6896e-02, -7.4623e-03, -2.7355e-02,
          -6.9043e-02,  1.0177e-01, -1.8675e-02,  3.6876e-02,  3.1893e-02,
           1.1653e-02, -6.5330e-02, -3.5094e-02, -3.6049e-02, -2.5020e-03,
           1.0195e-01, -6.7625e-02, -3.0182e-02,  4.4522e-02, -8.6149e-03,
           8.4931e-03, -1.1874e-01,  6.9026e-02,  1.5812e-03, -5.5509e-02,
           1.3606e-02,  5.0243e-02,  3.6947e-02,  1.0593e-01,  1.1874e-02,
           3.2183e-02,  1.3973e-02,  1.3894e-02, -1.7664e-02, -4.3989e-03,
          -5.1895e-02, -2.6888e-02,  5.2256e-03,  3.3785e-02, -8.8614e-03,
           9.7637e-02, -2.4016e-02, -1.6186e-02,  1.2086e-01, -1.8653e-02,
          -3.4667e-02,  2.5789e-02, -3.0697e-02,  6.0690e-02, -8.3111e-02,
          -1.2642e-02, -7.7414e-03,  5.5570e-03, -6.0138e-02, -3.4668e-02,
           1.1345e-03, -8.5204e-02, -2.3432e-02, -6.9470e-04,  1.6677e-02,
          -1.8019e-02, -4.5337e-02,  1.8311e-02,  9.2650e-03,  6.0911e-02]],

        [[ 1.7304e-02,  1.7189e-01,  6.5090e-02, -3.6988e-02,  1.1051e-02,
           2.5611e-03, -2.7772e-03, -7.9212e-02,  5.8201e-02,  7.2220e-03,
           9.1643e-02,  4.8543e-02,  7.6497e-03,  2.8169e-02,  3.3398e-02,
           4.1339e-03, -4.7870e-02,  1.0698e-01, -6.4281e-02,  2.0496e-02,
           7.4903e-03,  9.1508e-02,  2.2136e-02,  5.4230e-02,  1.5840e-02,
          -2.3598e-02,  6.4300e-02,  2.1576e-02, -4.8013e-02,  4.9337e-02,
           3.2911e-02,  3.7220e-02,  1.1378e-02,  6.6828e-02, -8.4393e-02,
           3.2075e-02, -2.7297e-02,  1.0327e-01, -8.6019e-03, -3.2338e-02,
          -8.0618e-02,  1.2326e-01, -2.2991e-02,  4.6712e-02,  4.2025e-02,
           1.0447e-02, -7.4340e-02, -4.3479e-02, -4.3846e-02,  3.1708e-03,
           1.2035e-01, -8.1922e-02, -3.4024e-02,  4.9949e-02, -1.2978e-02,
           1.4241e-02, -1.3645e-01,  8.1856e-02,  6.0250e-03, -6.3753e-02,
           9.3987e-03,  5.7154e-02,  4.6463e-02,  1.2631e-01,  1.8139e-02,
           4.1293e-02,  1.9405e-02,  1.0586e-02, -2.5104e-02, -9.4938e-04,
          -5.6451e-02, -2.7401e-02,  1.1283e-02,  3.1413e-02, -5.0195e-03,
           1.1556e-01, -2.5456e-02, -2.0909e-02,  1.4156e-01, -1.4050e-02,
          -4.1561e-02,  2.9938e-02, -3.5817e-02,  7.0725e-02, -9.8520e-02,
          -1.6896e-02, -1.3168e-02,  1.2783e-02, -6.8459e-02, -3.9180e-02,
           2.5707e-03, -9.7488e-02, -2.9469e-02, -6.0998e-03,  1.4238e-02,
          -1.6115e-02, -5.5598e-02,  2.4687e-02,  5.1693e-03,  7.1148e-02]]],
       device='cuda:0', grad_fn=<StackBackward>)
Final vector after FC layer
tensor([[0.9793, 0.0194, 0.0013]], device='cuda:0', grad_fn=<SoftmaxBackward>)
/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:63: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Negative
```

#### Example 2 -> "I like it very much."
```
classify_tweet("I like it very much.")

Encoder LSTM output vector after each word: 
after 1 word
tensor([-1.9363e-02,  1.8065e-02, -2.8903e-01,  4.0063e-02, -2.0260e-01,
         3.2156e-01,  9.2181e-02, -1.5146e-01, -3.6137e-02,  1.5422e-01,
        -5.0807e-02, -1.9956e-01,  2.6497e-01,  1.5487e-02,  1.6894e-01,
         1.8305e-02,  1.0102e-01,  1.3011e-01, -1.0462e-01,  3.6302e-02,
         6.3942e-02,  7.4309e-02,  6.4892e-02,  9.1315e-02,  1.5883e-01,
         3.5116e-01, -2.5601e-01,  3.1382e-02, -2.2766e-01,  8.7207e-04,
        -5.7092e-02, -9.9095e-02,  2.8005e-01, -3.9773e-01, -1.5462e-01,
        -8.1478e-02, -1.6369e-01,  1.9323e-01,  7.7096e-02, -1.5990e-01,
         3.2599e-01,  2.4637e-01, -9.6007e-02,  6.7740e-02,  1.1701e-01,
         5.0809e-01, -1.0506e-01, -2.0883e-02, -1.6657e-01,  5.9626e-01,
        -1.0922e-01, -7.0369e-03,  2.6509e-02, -2.1007e-02, -1.2084e-01,
        -4.1167e-01, -4.7511e-01,  2.8953e-02, -2.7167e-01, -9.7713e-02,
         6.2128e-03, -2.2218e-02,  3.3583e-02,  2.4181e-01,  5.1449e-02,
        -5.9765e-02,  3.1149e-01,  2.2281e-01, -4.2097e-03, -2.1404e-01,
        -8.9779e-02, -1.8694e-01, -1.2154e-01,  3.0281e-02, -1.2635e-01,
        -2.5292e-02, -1.4899e-02, -2.5745e-03,  1.0798e-01, -5.2243e-02,
        -1.7650e-01,  2.5863e-02,  2.0782e-01,  2.7208e-01, -1.4013e-02,
         2.0753e-04, -2.4969e-02, -1.6024e-01, -3.4437e-01, -1.5458e-01,
         1.3587e-01, -2.1985e-01, -3.0621e-01, -8.6799e-02, -5.3650e-02,
         7.7661e-02,  7.5306e-03, -4.7184e-02, -5.0823e-02, -4.4054e-02],
       device='cuda:0', grad_fn=<SelectBackward>)
after 2 word
tensor([ 0.0809, -0.0756, -0.4164, -0.0492,  0.1896,  0.2225, -0.0612,  0.0253,
         0.0291,  0.1428, -0.0743,  0.1664, -0.0037,  0.0272,  0.0666,  0.3114,
         0.3364, -0.1628,  0.1058, -0.2256,  0.1556,  0.0041, -0.1045,  0.0432,
         0.1404,  0.2489,  0.2233,  0.0685,  0.0547, -0.2004, -0.1620,  0.0848,
         0.3504,  0.1639, -0.3013,  0.2022, -0.1998, -0.0900,  0.1786, -0.1165,
        -0.2212, -0.0581, -0.0861,  0.0849, -0.0614,  0.2128,  0.1019,  0.0009,
        -0.1964,  0.0944, -0.1957, -0.1545,  0.0394, -0.1687, -0.3865, -0.0892,
        -0.0216, -0.0910,  0.0569,  0.0113,  0.1216,  0.2446,  0.0990,  0.0842,
         0.0933,  0.0612,  0.1373,  0.2890,  0.0459, -0.0022,  0.0080,  0.1086,
         0.0049,  0.0747, -0.2584,  0.0044, -0.0669, -0.1249,  0.0408,  0.1603,
        -0.0732, -0.0754,  0.0358, -0.0732, -0.0215,  0.2233,  0.0814,  0.0118,
        -0.1664, -0.2117,  0.0672, -0.1078, -0.0929,  0.0955, -0.1588,  0.1959,
        -0.0235, -0.0238,  0.2333, -0.0995], device='cuda:0',
       grad_fn=<SelectBackward>)
after 3 word
tensor([-1.0287e-02, -2.3382e-02, -1.9425e-01, -2.0313e-01,  3.9237e-01,
         4.1027e-01, -2.3121e-02, -1.2563e-01, -3.1842e-02,  3.2662e-01,
        -3.1010e-01,  1.1768e-01,  7.7153e-02, -4.0468e-01,  1.0051e-01,
         4.5601e-03,  4.4709e-02, -4.1517e-02, -1.5120e-01, -2.0825e-01,
         3.9881e-02, -9.8488e-02, -3.5766e-02,  3.1757e-01, -1.3617e-01,
         2.3583e-01,  4.8552e-02,  5.8394e-02,  1.0938e-01, -2.2215e-02,
        -2.8567e-01,  5.7933e-02,  2.8625e-01,  1.5631e-01, -5.7600e-02,
         1.8662e-01, -5.8870e-01, -2.9145e-01,  3.3126e-01, -1.0482e-01,
        -4.1460e-01, -3.1001e-04, -9.8844e-02, -2.1695e-01,  3.9593e-03,
         3.0160e-01,  3.2078e-01, -4.2862e-01, -2.3858e-01, -1.8248e-01,
         1.3750e-01, -3.6389e-01,  1.2620e-01, -5.0025e-01, -7.9373e-02,
         1.0314e-01, -1.5105e-01, -9.9992e-02, -5.5192e-03, -9.1239e-02,
        -9.9288e-02,  3.6746e-01,  6.6108e-05,  1.7794e-01,  3.4217e-01,
        -1.3677e-02, -1.9866e-01, -9.9723e-02, -4.9367e-03, -1.0626e-01,
         8.8455e-02, -2.0044e-02,  3.3482e-02, -1.7231e-01,  8.0966e-02,
        -2.1192e-01, -7.7007e-02,  8.8654e-02,  1.1156e-01,  5.8046e-02,
        -9.0713e-02, -1.1529e-01,  8.4466e-02, -3.4996e-01, -1.9207e-02,
         3.9930e-01, -3.8739e-02,  7.9585e-02,  1.5117e-01, -2.6014e-02,
        -2.4841e-01, -1.3859e-01,  3.4836e-02,  2.1761e-01,  8.2728e-02,
         9.2279e-02, -2.2678e-01, -5.3533e-02,  2.0308e-02, -7.4048e-02],
       device='cuda:0', grad_fn=<SelectBackward>)
after 4 word
tensor([ 0.1722,  0.0050, -0.0606, -0.0614,  0.0429,  0.3247,  0.0503, -0.0661,
         0.0845,  0.0366, -0.1218,  0.1842,  0.4422, -0.3141,  0.2735, -0.0608,
        -0.0204, -0.1170, -0.0215,  0.0409, -0.0021, -0.2725, -0.2127,  0.2298,
        -0.1109, -0.1071, -0.0160, -0.1130, -0.1670, -0.1453, -0.0678,  0.1608,
         0.0319, -0.0509,  0.0566,  0.0117, -0.4786, -0.1510,  0.1388, -0.0947,
         0.1334,  0.1592,  0.0214,  0.0390, -0.1816,  0.2224,  0.3553, -0.1764,
        -0.2297, -0.0790,  0.1315,  0.1962,  0.1118, -0.3124, -0.1525,  0.4206,
         0.0501, -0.1148,  0.1693, -0.1643, -0.0496,  0.2448,  0.0679,  0.0860,
         0.0397, -0.1327, -0.1987,  0.0743, -0.1944,  0.0568, -0.0196, -0.0575,
        -0.1533, -0.3134,  0.1392, -0.0994, -0.2680,  0.3783, -0.0244,  0.0480,
         0.2092, -0.1439,  0.1369, -0.0631,  0.1202,  0.1461,  0.0055,  0.1833,
         0.2659, -0.1368, -0.1284,  0.0104, -0.0137, -0.1092,  0.1739,  0.2621,
        -0.2356, -0.0309, -0.1405, -0.2115], device='cuda:0',
       grad_fn=<SelectBackward>)
after 5 word
tensor([ 0.4321,  0.0145,  0.1101,  0.0436,  0.1767,  0.1031, -0.0199, -0.0291,
         0.1898,  0.4281,  0.1298, -0.0180,  0.4293, -0.0693,  0.2188, -0.2196,
         0.1051,  0.0888, -0.1349,  0.0423, -0.1382, -0.0262, -0.4738,  0.1662,
        -0.3720, -0.0285,  0.3304, -0.0193, -0.0261, -0.2810, -0.0295,  0.1332,
        -0.0301,  0.0267,  0.0695,  0.1942, -0.0260, -0.1106,  0.1104, -0.1944,
         0.2592,  0.1525, -0.0495,  0.1022, -0.2033,  0.1017,  0.3017, -0.0527,
         0.0243, -0.1790,  0.0739,  0.1331, -0.0267, -0.0612, -0.2545,  0.2155,
         0.1489, -0.0372,  0.0960, -0.3070,  0.0714,  0.1006,  0.0340, -0.0303,
         0.2120, -0.0158, -0.2511,  0.0349, -0.0415, -0.0225, -0.1504, -0.0216,
        -0.0734, -0.1232, -0.0209,  0.0897, -0.1610,  0.2647, -0.2608, -0.0072,
         0.0854, -0.3164,  0.0674, -0.1835,  0.4256,  0.1499, -0.0389, -0.0041,
         0.0729,  0.1748, -0.4450, -0.2130,  0.4384,  0.0490,  0.1443, -0.0671,
        -0.3519, -0.0101, -0.3242, -0.2033], device='cuda:0',
       grad_fn=<SelectBackward>)
after 6 word
tensor([ 4.6330e-01, -5.7365e-02,  3.6275e-01,  3.4664e-01,  1.7201e-01,
         1.5483e-01,  2.8086e-02,  1.4348e-01,  3.3998e-01,  2.9779e-01,
         3.5232e-01, -2.5766e-01,  1.3536e-02,  6.4634e-03,  3.0183e-01,
         1.8582e-02,  4.7873e-02, -6.6592e-02,  4.6898e-04, -3.0219e-01,
        -6.5842e-03,  1.3938e-01, -4.3574e-01,  2.9370e-01, -1.9766e-01,
        -3.0877e-01, -2.8220e-02, -2.3835e-01, -1.2415e-01, -9.3993e-02,
        -2.2884e-01,  1.1154e-01,  1.5078e-01, -1.9187e-02,  1.2395e-01,
        -1.3062e-01, -7.1355e-02,  2.2682e-02,  1.1379e-01, -4.2631e-01,
         4.2613e-03,  4.9109e-01,  2.5246e-01,  3.0634e-01, -3.0391e-01,
        -4.3549e-02,  1.8436e-01, -2.7516e-02,  1.6280e-01,  1.6568e-01,
         2.1982e-02, -3.1420e-02,  5.9351e-02, -1.8732e-01, -1.9853e-01,
         8.1403e-02,  2.1916e-01,  5.9751e-02,  5.9171e-01,  2.2646e-01,
         1.9182e-01,  1.0564e-01, -2.0306e-01, -7.8628e-02, -1.5749e-01,
        -1.9935e-01, -9.0324e-02,  1.5773e-01, -4.4678e-02, -1.0376e-01,
         3.1567e-02, -5.5934e-02, -1.6311e-01, -2.6795e-01, -1.2871e-01,
        -3.1675e-02, -6.9984e-02,  2.3054e-01, -6.3365e-03,  1.0971e-01,
         1.6506e-01, -2.8171e-03,  7.2873e-02, -3.8606e-01, -3.5412e-01,
         2.0335e-01, -1.0758e-01, -5.1112e-02,  2.2638e-02,  1.3109e-01,
        -3.3437e-01, -6.0154e-02,  1.7035e-01, -1.0982e-01,  6.4032e-02,
        -8.3931e-02,  1.7692e-02,  2.8192e-03, -1.8574e-01, -2.2678e-01],
       device='cuda:0', grad_fn=<SelectBackward>)

Encoder LSTM final hidden vector
tensor([[[ 4.6330e-01, -5.7365e-02,  3.6275e-01,  3.4664e-01,  1.7201e-01,
           1.5483e-01,  2.8086e-02,  1.4348e-01,  3.3998e-01,  2.9779e-01,
           3.5232e-01, -2.5766e-01,  1.3536e-02,  6.4634e-03,  3.0183e-01,
           1.8582e-02,  4.7873e-02, -6.6592e-02,  4.6898e-04, -3.0219e-01,
          -6.5842e-03,  1.3938e-01, -4.3574e-01,  2.9370e-01, -1.9766e-01,
          -3.0877e-01, -2.8220e-02, -2.3835e-01, -1.2415e-01, -9.3993e-02,
          -2.2884e-01,  1.1154e-01,  1.5078e-01, -1.9187e-02,  1.2395e-01,
          -1.3062e-01, -7.1355e-02,  2.2682e-02,  1.1379e-01, -4.2631e-01,
           4.2613e-03,  4.9109e-01,  2.5246e-01,  3.0634e-01, -3.0391e-01,
          -4.3549e-02,  1.8436e-01, -2.7516e-02,  1.6280e-01,  1.6568e-01,
           2.1982e-02, -3.1420e-02,  5.9351e-02, -1.8732e-01, -1.9853e-01,
           8.1403e-02,  2.1916e-01,  5.9751e-02,  5.9171e-01,  2.2646e-01,
           1.9182e-01,  1.0564e-01, -2.0306e-01, -7.8628e-02, -1.5749e-01,
          -1.9935e-01, -9.0324e-02,  1.5773e-01, -4.4678e-02, -1.0376e-01,
           3.1567e-02, -5.5934e-02, -1.6311e-01, -2.6795e-01, -1.2871e-01,
          -3.1675e-02, -6.9984e-02,  2.3054e-01, -6.3365e-03,  1.0971e-01,
           1.6506e-01, -2.8171e-03,  7.2873e-02, -3.8606e-01, -3.5412e-01,
           2.0335e-01, -1.0758e-01, -5.1112e-02,  2.2638e-02,  1.3109e-01,
          -3.3437e-01, -6.0154e-02,  1.7035e-01, -1.0982e-01,  6.4032e-02,
          -8.3931e-02,  1.7692e-02,  2.8192e-03, -1.8574e-01, -2.2678e-01]]],
       device='cuda:0', grad_fn=<CudnnRnnBackward>)

Decoder:  tensor([[[ 1.6095e-02,  7.3382e-02, -4.0703e-02, -2.9712e-02,  3.4677e-02,
          -2.5587e-02, -1.1707e-04,  5.7506e-02,  5.9566e-02,  7.3761e-03,
           1.5144e-03, -2.4551e-02, -4.4281e-03, -6.2318e-02, -4.9065e-02,
           4.9025e-02, -2.6104e-02,  8.5640e-03, -1.7301e-04,  2.2944e-02,
           1.9325e-02,  2.3903e-03,  6.1068e-02,  1.9932e-02,  4.6435e-03,
          -4.4239e-03,  1.3815e-02,  2.5735e-02, -8.7019e-03,  3.3814e-02,
           5.5071e-02, -3.5011e-02,  1.3302e-02,  6.8814e-03,  2.1302e-02,
          -1.9309e-02,  5.9525e-03,  1.5182e-03, -1.4800e-02, -4.6050e-02,
          -6.3187e-03, -2.5736e-02, -2.3927e-03, -4.5079e-02,  5.0297e-03,
           1.6786e-02, -2.6880e-02,  5.1260e-03, -2.1281e-02, -4.1842e-02,
          -4.1102e-03, -3.3027e-03,  1.6171e-02, -2.9512e-02, -6.2946e-02,
           1.8723e-02,  1.8730e-02, -1.8220e-02,  1.9788e-02,  9.3752e-03,
           3.8677e-02, -9.1923e-02, -1.3530e-02, -3.5586e-02, -1.6809e-02,
          -1.8967e-02, -3.7189e-02, -2.4076e-02, -9.7864e-03, -6.1625e-02,
           1.6187e-03,  2.6884e-02, -2.4650e-02, -2.9178e-02,  5.0470e-02,
           2.4528e-02, -4.2726e-02, -1.0231e-02, -7.4774e-03,  7.7806e-02,
          -1.1815e-02, -4.7041e-02, -2.1498e-02,  4.4137e-02,  5.5163e-02,
           6.0509e-02, -1.9707e-03,  2.8418e-02, -3.2964e-02, -7.4499e-02,
           5.1850e-02,  5.1660e-02, -1.0544e-02, -2.3180e-02,  4.8777e-02,
          -2.9888e-02, -3.1651e-02, -1.4336e-02, -3.1557e-02, -1.7349e-02]],

        [[ 2.4992e-02,  1.0913e-01, -6.2964e-02, -3.6980e-02,  5.4587e-02,
          -3.7287e-02,  1.1509e-02,  9.1131e-02,  9.3568e-02,  9.1548e-03,
           4.5498e-03, -4.2583e-02, -5.3297e-05, -9.7166e-02, -6.5125e-02,
           7.2232e-02, -3.5865e-02,  1.9060e-02, -8.0841e-03,  3.4721e-02,
           3.5378e-02,  5.9681e-03,  9.5006e-02,  3.3479e-02,  1.1474e-02,
          -7.6805e-03,  1.6997e-02,  3.6560e-02, -1.2233e-02,  5.4142e-02,
           7.9449e-02, -5.1549e-02,  2.1059e-02,  1.2426e-02,  2.0415e-02,
          -3.1258e-02,  1.1095e-02,  2.2293e-04, -1.7520e-02, -7.7389e-02,
          -1.3259e-02, -4.1458e-02, -1.4295e-03, -6.4256e-02,  1.0536e-02,
           2.6764e-02, -3.5248e-02,  7.5457e-03, -2.7439e-02, -6.2439e-02,
          -1.0579e-02, -2.9867e-03,  2.7252e-02, -4.5466e-02, -9.9994e-02,
           2.7708e-02,  2.2384e-02, -2.1101e-02,  3.7504e-02,  5.3242e-03,
           7.0102e-02, -1.3166e-01, -2.6578e-02, -5.0120e-02, -2.5320e-02,
          -2.9104e-02, -5.6392e-02, -3.8523e-02, -2.1353e-02, -9.3230e-02,
           6.1771e-03,  4.0941e-02, -4.2496e-02, -4.5192e-02,  6.3121e-02,
           4.4871e-02, -6.7278e-02, -1.2382e-02, -9.9716e-03,  1.1718e-01,
          -1.5610e-02, -6.8611e-02, -3.0733e-02,  6.6031e-02,  8.7965e-02,
           9.6009e-02, -4.6556e-03,  4.0545e-02, -5.2620e-02, -1.1226e-01,
           8.0723e-02,  7.7913e-02, -1.0407e-02, -3.9333e-02,  7.1112e-02,
          -4.8778e-02, -4.8095e-02, -2.3558e-02, -4.9102e-02, -2.5556e-02]],

        [[ 2.9380e-02,  1.2534e-01, -7.4757e-02, -3.5824e-02,  6.5996e-02,
          -4.3246e-02,  2.1796e-02,  1.1043e-01,  1.1238e-01,  9.6767e-03,
           7.1106e-03, -5.5945e-02,  4.9464e-03, -1.1661e-01, -6.9589e-02,
           8.3528e-02, -4.0033e-02,  2.7885e-02, -1.6475e-02,  4.1064e-02,
           4.7267e-02,  8.7521e-03,  1.1400e-01,  4.1740e-02,  1.6922e-02,
          -1.0434e-02,  1.5562e-02,  4.1883e-02, -1.3047e-02,  6.7031e-02,
           8.8782e-02, -6.0284e-02,  2.4647e-02,  1.5150e-02,  1.3191e-02,
          -3.8182e-02,  1.5067e-02, -9.5263e-04, -1.5518e-02, -9.6725e-02,
          -1.8028e-02, -5.1476e-02,  8.1733e-04, -7.1713e-02,  1.4991e-02,
           3.2342e-02, -3.8123e-02,  7.7791e-03, -2.8707e-02, -7.2492e-02,
          -1.6152e-02, -2.6819e-03,  3.4148e-02, -5.4157e-02, -1.2215e-01,
           3.2455e-02,  2.0663e-02, -2.0337e-02,  5.0505e-02, -2.2285e-03,
           9.1279e-02, -1.4877e-01, -3.5872e-02, -5.5995e-02, -3.0571e-02,
          -3.3973e-02, -6.6663e-02, -4.7689e-02, -3.0645e-02, -1.0992e-01,
           1.0795e-02,  4.9169e-02, -5.5463e-02, -5.3165e-02,  6.3951e-02,
           5.9642e-02, -8.0161e-02, -1.2185e-02, -1.1263e-02,  1.3763e-01,
          -1.6560e-02, -7.7546e-02, -3.4593e-02,  7.7050e-02,  1.0698e-01,
           1.1642e-01, -6.4258e-03,  4.5784e-02, -6.2067e-02, -1.3100e-01,
           9.5853e-02,  9.1360e-02, -8.0975e-03, -5.0595e-02,  8.1005e-02,
          -5.9818e-02, -5.6962e-02, -3.0064e-02, -5.8083e-02, -2.8974e-02]]],
       device='cuda:0', grad_fn=<StackBackward>)
Final vector after FC layer
tensor([[0.0431, 0.9544, 0.0025]], device='cuda:0', grad_fn=<SoftmaxBackward>)
/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:63: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Positive
```
